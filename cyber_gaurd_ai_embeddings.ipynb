{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "from collections import Counter \n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>READ<<<<: \n",
    "# ADD GOOGLE API KEY in ./.env file\n",
    "# GOOGLE_API_KEY=...\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd.read_csv(\"./test.csv\")\n",
    "# prepare google genai\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "# prepare groq client\n",
    "groq_client = openai.OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ[\"GROQ_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for embeddings\n",
    "# new doc\n",
    "def mod_func(row):\n",
    "    row['document'] = f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\n",
    "    \"\"\"\n",
    "    return row\n",
    "\n",
    "train_data['sub_category'] = train_data['sub_category'].fillna(train_data['category'])\n",
    "# prepared_train_data =  train_data.apply(mod_func, axis=1)\n",
    "\n",
    "train_data['combined_length'] = train_data['category'].fillna('').str.len() + \\\n",
    "                        train_data['sub_category'].fillna('').str.len() + \\\n",
    "                        train_data['crimeaditionalinfo'].fillna('').str.len()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that if any row exceeds embeddings context length\n",
    "embeddings_context_length = 2048\n",
    "if len(train_data[train_data.combined_length > embeddings_context_length]) > 1:\n",
    "    train_data[train_data.combined_length > embeddings_context_length]\n",
    "else:\n",
    "    print(\"Success! All rows are within the context length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vectordb\n",
    "\n",
    "# start chromdb server\n",
    "# chroma run --host localhost --port 8000 --path ./cyber_gaurd_ai_embeddings_vector_data\n",
    "\n",
    "embeddings_collection = None\n",
    "def collection_exists(collection_name):\n",
    "    existing_collections = client.list_collections()\n",
    "    return any(collection.name == collection_name for collection in existing_collections)\n",
    "\n",
    "client = chromadb.HttpClient()\n",
    "collection_name = \"cyber-ai-embeddings\"\n",
    "summarised_collection_name = \"cyber-ai-embeddings-summed\"\n",
    "\n",
    "if not collection_exists(collection_name):\n",
    "    embeddings_collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    embeddings_collection = client.get_collection(collection_name)\n",
    "    \n",
    "# summarised collection\n",
    "\n",
    "if not collection_exists(summarised_collection_name):\n",
    "    embeddings_collection_summed = client.create_collection(summarised_collection_name)\n",
    "else:\n",
    "    embeddings_collection_summed = client.get_collection(summarised_collection_name)\n",
    "    \n",
    "    \n",
    "print(\"Collection created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categories(document,sample_size = 1,collection=embeddings_collection):\n",
    "    predicted_category = \"\"\n",
    "    predicted_scategory = \"\"\n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=[document])\n",
    "    test_embeddings = result['embedding'][0]\n",
    "    # query db\n",
    "    query_result = collection.query(query_embeddings=test_embeddings,n_results=sample_size)\n",
    "    # print results\n",
    "    sampled_categories = []\n",
    "    sampled_scategories = []\n",
    "    for i in range(sample_size):\n",
    "        if 'category' in query_result['metadatas'][0][0]:\n",
    "            sampled_categories.append(query_result['metadatas'][0][0]['category'])\n",
    "        else:\n",
    "            sampled_categories.append(None)\n",
    "    \n",
    "    # get the most occurred\n",
    "    predicted_category = Counter(sampled_categories).most_common(1)[0][0]\n",
    "    \n",
    "    # get sub category\n",
    "    # query db\n",
    "    query_result = collection.query(query_embeddings=test_embeddings,n_results=sample_size,where={\"category\":predicted_category})\n",
    "       \n",
    "    if 'scategory' in query_result['metadatas'][0][0]:\n",
    "        sampled_scategories.append(query_result['metadatas'][0][0]['scategory'])\n",
    "    else:\n",
    "        sampled_scategories.append(None)\n",
    "\n",
    "    predicted_scategory = Counter(sampled_scategories).most_common(1)[0][0]\n",
    "    return (predicted_category,predicted_scategory,sampled_categories,sampled_scategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "# >>>>PLEASE READ!!!!!<<<<\n",
    "# THIS CELL EXECUTION SHOULD BE A ONE TIME PROCESS\n",
    "# REMOVE THE ABOVE \"%%SCRIPT TRUE\" TO RUN THIS CELL\n",
    "# THIS CELL PREPARES AND POPULATES ALL THE VECTOR EMBEDDINGS OF THE TRAIN DATA\n",
    "# \n",
    "step = 200\n",
    "max_length = len(train_data)\n",
    "for i in range(0, max_length, step):\n",
    "    start_idx = i\n",
    "    end_idx = i + step\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    document_ids = []\n",
    "    for index, row in train_data[start_idx:end_idx].iterrows():\n",
    "        doc_ = f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "        documents.append(doc_)\n",
    "        metadatas.append({\"category\": row.category, \"scategory\": row.sub_category})\n",
    "        document_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=documents)\n",
    "\n",
    "    print(\"inserting results in vectordb\")\n",
    "    embeddings = result[\"embedding\"]\n",
    "    data_to_insert = embeddings_collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=document_ids,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f\"data inserted from index {start_idx} to {end_idx}\")\n",
    "print(\"Vector embeddings prepared...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SINGLE\n",
    "test_idx = int(len(test_data)*random.random())\n",
    "row = test_data.iloc[test_idx]\n",
    "print(f\"Expected Category: {row.category}\")\n",
    "print(f\"Expected Sub-category: {row.sub_category}\")\n",
    "print(f\"Complaint: {row.crimeaditionalinfo}\")\n",
    "print()\n",
    "predict_category,predict_scategory,sampled_categories,sampled_scategories  = predict_categories(f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\",10)\n",
    "print(sampled_categories)\n",
    "print(sampled_scategories)\n",
    "print()\n",
    "print(f\"Predicted Category: {predict_category}\")\n",
    "print(f\"Predicted Sub-category: {predict_scategory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ALL\n",
    "sample_size = 10\n",
    "test_test_data = test_data.sample(n=sample_size).reset_index(drop=True)\n",
    "failure_indexes = []\n",
    "failure_indexes_category = []\n",
    "failure_indexes_scategory = []\n",
    "failure_predicted_categories = []\n",
    "for index, row in test_test_data.iterrows():\n",
    "    print(f\"{index+1}/{sample_size}\",end='\\r')\n",
    "    document = f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "    predict_category,predict_scategory,sampled_categories,sampled_scategories = predict_categories(document,10)\n",
    "    \n",
    "    if row.category == predict_category:\n",
    "        if row.sub_category == predict_scategory or pd.isna(row.sub_category) or row.sub_category == None:\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            # print(\"Failed Sub-category---------\")\n",
    "            # print(f\"Index: {index}\")\n",
    "            # print(f\"Expected: {row.sub_category}\")\n",
    "            # print(f\"Predicted: {predict_scategory}\")\n",
    "            # print(\"----------------------------\")\n",
    "            failure_indexes_scategory.append(index)\n",
    "    else:\n",
    "        print(\"Failed Category-------------\")\n",
    "        print(f\"Index: {index}\")\n",
    "        print(f\"Expected: {row.category}\")\n",
    "        print(f\"Predicted: {predict_category}\")\n",
    "        print(\"----------------------------\")\n",
    "        failure_indexes_category.append(index)\n",
    "    \n",
    "    failure_predicted_categories.append([predict_category,predict_scategory])\n",
    "    failure_indexes.append(index)\n",
    "        \n",
    "print(f\"Success Rate Overall: {100 - len(failure_indexes)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Category only): {100 - len(failure_indexes_category)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Sub-category only): {100 - len(failure_indexes_scategory)/sample_size*100}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarised Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_paragraph(category,sub_category,sample_size=10):\n",
    "    cat_1 = train_data[(train_data.category == category) & (train_data.sub_category == sub_category)].sample(n=sample_size).reset_index(drop=True)\n",
    "    complaints = \"\"\n",
    "    for index, row in cat_1.iterrows(): \n",
    "        if pd.isna(row.crimeaditionalinfo) or row.crimeaditionalinfo == None:\n",
    "            continue\n",
    "        complaints += f\"\\nComplaint {index+1}:\\n\" + row.crimeaditionalinfo + \"\\n\\n\"\n",
    "        \n",
    "    # processed = f\"\"\"Following are the user written complaints. Please tranform all the complaints to paragraphs such that it retains a general idea about the complaints and is ideal for vector embeddings. Please respond only with text, no markdown.\n",
    "    #     {complaints}\n",
    "    # \"\"\"\n",
    "\n",
    "    processed = f\"\"\"You are a cybersecurity incident analyst tasked with summarizing (in pure text) multiple user complaints about potential cyber issues written in english or hinglish. The complaints are written by users in India and the context of the complains are exclusively Indian. Your goal is to create a concise summary that captures the main concerns and patterns across all complaints while retaining the essential details.\n",
    "\n",
    "Please provide a summary of the following user complaints, highlighting:\n",
    "\n",
    "1. Common issues or themes\n",
    "2. Specific types of cyber threats mentioned\n",
    "3. Any patterns in affected systems or devices\n",
    "4. Potential impact on users or organizations\n",
    "5. Any unusual or standout complaints\n",
    "\n",
    "Instructions to follow:\n",
    "1. Please respond only with text, NO markdown.\n",
    "2. If the language is not english then preserve the language elements.\n",
    "\n",
    "Combine similar complaints and present the information in a clear, organized manner. Your summary should give a comprehensive overview of the situation without losing important details from individual complaints. \n",
    "\n",
    "User complaints:\n",
    "\n",
    "{complaints}\n",
    "    \"\"\"\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genai_predict(input_text):\n",
    "    # system_instruction = \"Do not respond in markdown, respond only with pure text, it can have paragraphs.\"\n",
    "    system_instruction = \"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    response = model.generate_content(input_text)\n",
    "    res_content = response.candidates[0].content\n",
    "    res_part = res_content.parts[0]\n",
    "    res_text = res_part.text\n",
    "    res_text.replace(\"*\",\"\")\n",
    "    return res_text,response._error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_predict(input_text):\n",
    "    # system_instruction = \"Do not respond in markdown, respond only with pure text, it can have paragraphs.\"\n",
    "    system_instruction = \"\"\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_instruction,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text,\n",
    "        },\n",
    "    ],\n",
    "    model=\"llama-3.2-90b-text-preview\",\n",
    ")\n",
    "    res_text = chat_completion.choices[0].message.content\n",
    "    res_text.replace(\"*\",\"\")\n",
    "    return res_text,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_summarise_categories(category,sub_category,sample_size=10,llm_channel=\"genai\"):\n",
    "    \n",
    "    if llm_channel == \"genai\":\n",
    "        summary_,err = genai_predict(prepare_paragraph(category,sub_category))\n",
    "    elif llm_channel == \"groq\":\n",
    "        summary_,err = groq_predict(prepare_paragraph(category,sub_category))\n",
    "    \n",
    "    if err is not None:\n",
    "        print(err)\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    document_ids = []\n",
    "\n",
    "    metadatas.append({\"category\": category, \"scategory\": sub_category})\n",
    "    documents.append(summary_)\n",
    "    document_ids.append(str(uuid.uuid4()))\n",
    "    \n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=documents)\n",
    "    \n",
    "    print(\"inserting results in summed collection\")\n",
    "    embeddings = result[\"embedding\"]\n",
    "    embeddings_collection_summed.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=document_ids,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f\"data inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script true\n",
    "# >>>>PLEASE READ!!!!!<<<<\n",
    "# THIS CELL EXECUTION SHOULD BE A ONE TIME PROCESS\n",
    "# REMOVE THE ABOVE \"%%SCRIPT TRUE\" TO RUN THIS CELL\n",
    "# THIS CELL PREPARES AND POPULATES ALL THE VECTOR EMBEDDINGS OF THE TRAIN DATA\n",
    "# \n",
    "# # prepare data based on summarisation\n",
    "skip_categories = ['RapeGang Rape RGRSexually Abusive Content','Sexually Explicit Act','Sexually Obscene material','Child Pornography CPChild Sexual Abuse Material CSAM']\n",
    "categories = train_data.groupby('category')['sub_category'].agg(lambda x: sorted(set(x))).to_dict()\n",
    "# sub_categories = train_data.sub_category.unique()\n",
    "failures = []\n",
    "\n",
    "for cat in categories:\n",
    "    # llm_channel = \"genai\"\n",
    "    llm_channel = \"groq\"\n",
    "    if cat in skip_categories:\n",
    "        llm_channel = \"groq\"\n",
    "    for scat in categories[cat]:\n",
    "        print(f\"Doing for {cat} and {scat} using {llm_channel} ...\")\n",
    "        try:\n",
    "            # do for 5 times\n",
    "            sample_and_summarise_categories(cat,scat,10,llm_channel=llm_channel)\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"FAILED!\")\n",
    "            failures.append((cat,scat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SINGLE\n",
    "test_idx = int(len(test_data)*random.random())\n",
    "row = test_data.iloc[test_idx]\n",
    "print(f\"Index: {test_idx}\")\n",
    "print(f\"Expected Category: {row.category}\")\n",
    "print(f\"Expected Sub-category: {row.sub_category}\")\n",
    "print(f\"Complaint: {row.crimeaditionalinfo}\")\n",
    "print()\n",
    "predict_category,predict_scategory,sampled_categories,sampled_scategories  = predict_categories(f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\",10,collection=embeddings_collection_summed)\n",
    "print(sampled_categories)\n",
    "print(sampled_scategories)\n",
    "print()\n",
    "print(f\"Predicted Category: {predict_category}\")\n",
    "print(f\"Predicted Sub-category: {predict_scategory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ALL\n",
    "sample_size = 100\n",
    "test_test_data = test_data.sample(n=sample_size).reset_index(drop=True)\n",
    "failure_indexes = []\n",
    "failure_indexes_category = []\n",
    "failure_indexes_scategory = []\n",
    "failure_predicted_categories = []\n",
    "for index, row in test_test_data.iterrows():\n",
    "    print(f\"{index+1}/{sample_size}\",end='\\r')\n",
    "    document = f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "    predict_category,predict_scategory,sampled_categories,sampled_scategories = predict_categories(document,10,collection=embeddings_collection_summed)\n",
    "    \n",
    "    if row.category == predict_category:\n",
    "        if row.sub_category == predict_scategory or pd.isna(row.sub_category) or row.sub_category == None:\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            # print(\"Failed Sub-category---------\")\n",
    "            # print(f\"Index: {index}\")\n",
    "            # print(f\"Expected: {row.sub_category}\")\n",
    "            # print(f\"Predicted: {predict_scategory}\")\n",
    "            # print(\"----------------------------\")\n",
    "            failure_indexes_scategory.append(index)\n",
    "    else:\n",
    "        print(\"Failed Category-------------\")\n",
    "        print(f\"Index: {index}\")\n",
    "        print(f\"Expected: {row.category}\")\n",
    "        print(f\"Predicted: {predict_category}\")\n",
    "        print(\"----------------------------\")\n",
    "        failure_indexes_category.append(index)\n",
    "    \n",
    "    failure_predicted_categories.append([predict_category,predict_scategory])\n",
    "    failure_indexes.append(index)\n",
    "        \n",
    "print(f\"Success Rate Overall: {100 - len(failure_indexes)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Category only): {100 - len(failure_indexes_category)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Sub-category only): {100 - len(failure_indexes_scategory)/sample_size*100}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_data.iloc[2].crimeaditionalinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = train_data.groupby('category')['sub_category'].agg(lambda x: sorted(set(x))).to_dict()\n",
    "del categories['Any Other Cyber Crime'] # TODO: !!???\n",
    "\n",
    "# categories = {\"Child Pornography CPChild Sexual Abuse Material CSAM\":['Child Pornography CPChild Sexual Abuse Material CSAM']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "# >>>>PLEASE READ!!!!!<<<<\n",
    "# THIS CELL EXECUTION SHOULD BE A ONE TIME PROCESS\n",
    "# REMOVE THE ABOVE \"%%SCRIPT TRUE\" TO RUN THIS CELL\n",
    "# THIS CELL PREPARES AND POPULATES ALL THE AVERGAE VECTOR EMBEDDINGS OF THE TRAIN DATA\n",
    "# average_embeddings.json in the repo already contains the averaged data so no need to execute this cell\n",
    "# \n",
    "final_embeddings = {}\n",
    "step = 200\n",
    "for cat in categories:\n",
    "    for scat in categories[cat]:\n",
    "        print(f\"Doing for {cat} and {scat} ...\")\n",
    "        specific_data = train_data[(train_data.category == cat) & (train_data.sub_category == scat)]\n",
    "        max_length = len(specific_data)\n",
    "        embeddings_all = []\n",
    "        for i in range(0, max_length, step):\n",
    "            start_idx = i\n",
    "            end_idx = i + step\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            document_ids = []\n",
    "            print(f\"{start_idx}-{end_idx}/{max_length}\",end='\\r')\n",
    "            for index, row in specific_data[start_idx:end_idx].iterrows():\n",
    "                doc_ = f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "                documents.append(doc_)\n",
    "                metadatas.append({\"category\": row.category, \"scategory\": row.sub_category})\n",
    "                document_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "            result = genai.embed_content(model=\"models/text-embedding-004\", content=documents)\n",
    "\n",
    "            embeddings_all.extend(result['embedding'])\n",
    "        print()\n",
    "        # do mean\n",
    "        average_embeddings = np.mean(embeddings_all,axis=0)\n",
    "        if cat not in final_embeddings:\n",
    "            final_embeddings[cat] = {}\n",
    "        if scat not in final_embeddings[cat]:\n",
    "            final_embeddings[cat][scat] = []\n",
    "            \n",
    "        final_embeddings[cat][scat] = average_embeddings.tolist()\n",
    "    \n",
    "# save to file\n",
    "f = open(\"./average_embeddings.json\",'w')\n",
    "json.dump(final_embeddings,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_embeddings(query_embedding, embedding_list,n_sorted=10):\n",
    "    \"\"\"\n",
    "    Find the closest embeddings from a list of embeddings, sorted by similarity.\n",
    "    \n",
    "    Args:\n",
    "    query_embedding (np.array): The embedding to compare against.\n",
    "    embedding_list (list): List of embeddings to search through.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (closest_embedding, index, similarity_score, sorted_embeddings, sorted_indices)\n",
    "    \"\"\"\n",
    "    # Convert the query embedding to a 2D array\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    # Convert the list of embeddings to a 2D array\n",
    "    embedding_array = np.array(embedding_list)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding, embedding_array)[0]\n",
    "    \n",
    "    # Sort similarities in descending order and get sorted indices\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Get the closest embedding, its index, and similarity score\n",
    "    closest_index = sorted_indices[0]\n",
    "    closest_embedding = embedding_list[closest_index]\n",
    "    similarity_score = similarities[closest_index]\n",
    "    \n",
    "    # Create a list of tuples (embedding, original_index, similarity_score)\n",
    "    sorted_embeddings = [(embedding_list[i], i, similarities[i]) for i in sorted_indices]\n",
    "    \n",
    "    return closest_embedding, closest_index, similarity_score, sorted_embeddings[:n_sorted], sorted_indices[:n_sorted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read final embeddings and prepare list\n",
    "f = open(\"./average_embeddings.json\",'r')\n",
    "average_embeddings_dict = json.load(f)\n",
    "average_embeddings_embeddings_list = []\n",
    "average_embeddings_pair_list = []\n",
    "f.close()\n",
    "# prepare list and sub pairs\n",
    "for i in average_embeddings_dict:\n",
    "    for j in average_embeddings_dict[i]:\n",
    "        average_embeddings_embeddings_list.append(average_embeddings_dict[i][j])\n",
    "        average_embeddings_pair_list.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_average_embeddings(document,embeddings_list=average_embeddings_embeddings_list,pair_list=average_embeddings_pair_list,sample_size = 10):\n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=[document])\n",
    "    test_embeddings = result['embedding'][0]\n",
    "    closest_embedding, closest_index, similarity_score, sorted_embeddings, sorted_indices = get_closest_embeddings(test_embeddings,embeddings_list,sample_size)\n",
    "    \n",
    "    predicted_pairs = [pair_list[i] for i in sorted_indices]\n",
    "    \n",
    "    predicted_pair = Counter(predicted_pairs).most_common(1)[0][0]\n",
    "    \n",
    "    return predicted_pair\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SINGLE\n",
    "test_idx = int(len(test_data)*random.random())\n",
    "row = test_data.iloc[test_idx]\n",
    "print(f\"Index: {test_idx}\")\n",
    "print(f\"Expected Category: {row.category}\")\n",
    "print(f\"Expected Sub-category: {row.sub_category}\")\n",
    "print(f\"Complaint: {row.crimeaditionalinfo}\")\n",
    "print()\n",
    "predicted_pair  = predict_from_average_embeddings(f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\",sample_size=10)\n",
    "print()\n",
    "print(f\"Predicted Category: {predicted_pair[0]}\")\n",
    "print(f\"Predicted Sub-category: {predicted_pair[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ALL\n",
    "sample_size = 100\n",
    "test_test_data = test_data.sample(n=sample_size).reset_index(drop=True)\n",
    "failure_indexes = []\n",
    "failure_indexes_category = []\n",
    "failure_indexes_scategory = []\n",
    "failure_predicted_categories = []\n",
    "for index, row in test_test_data.iterrows():\n",
    "    print(f\"{index+1}/{sample_size}\",end='\\r')\n",
    "    document = f\"\"\"The following complaint document written by a user:\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "    predicted_pair = predict_from_average_embeddings(document,sample_size=10)\n",
    "    predict_category = predicted_pair[0]\n",
    "    predict_scategory= predicted_pair[1]\n",
    "    if row.category == predict_category:\n",
    "        if row.sub_category == predict_scategory or pd.isna(row.sub_category) or row.sub_category == None:\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            # print(\"Failed Sub-category---------\")\n",
    "            # print(f\"Index: {index}\")\n",
    "            # print(f\"Expected: {row.sub_category}\")\n",
    "            # print(f\"Predicted: {predict_scategory}\")\n",
    "            # print(\"----------------------------\")\n",
    "            failure_indexes_scategory.append(index)\n",
    "    else:\n",
    "        print(\"Failed Category-------------\")\n",
    "        print(f\"Index: {index}\")\n",
    "        print(f\"Expected: {row.category}\")\n",
    "        print(f\"Predicted: {predict_category}\")\n",
    "        print(\"----------------------------\")\n",
    "        failure_indexes_category.append(index)\n",
    "    \n",
    "    failure_predicted_categories.append([predict_category,predict_scategory])\n",
    "    failure_indexes.append(index)\n",
    "        \n",
    "print(f\"Success Rate Overall: {100 - len(failure_indexes)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Category only): {100 - len(failure_indexes_category)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Sub-category only): {100 - len(failure_indexes_scategory)/sample_size*100}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
