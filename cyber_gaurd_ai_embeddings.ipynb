{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "from collections import Counter \n",
    "import random\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>READ<<<<: \n",
    "# ADD GOOGLE API KEY in ./.env file\n",
    "# GOOGLE_API_KEY=...\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd.read_csv(\"./test.csv\")\n",
    "# prepare google genai\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for embeddings\n",
    "# new doc\n",
    "def mod_func(row):\n",
    "    row['document'] = f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\n",
    "    \"\"\"\n",
    "    return row\n",
    "\n",
    "prepared_train_data =  train_data.apply(mod_func, axis=1)\n",
    "prepared_train_data['combined_length'] = prepared_train_data['category'].fillna('').str.len() + \\\n",
    "                        prepared_train_data['sub_category'].fillna('').str.len() + \\\n",
    "                        prepared_train_data['crimeaditionalinfo'].fillna('').str.len()\n",
    "\n",
    "prepared_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that if any row exceeds embeddings context length\n",
    "embeddings_context_length = 2048\n",
    "if len(prepared_train_data[prepared_train_data.combined_length > embeddings_context_length]) > 1:\n",
    "    prepared_train_data[prepared_train_data.combined_length > embeddings_context_length]\n",
    "else:\n",
    "    print(\"Success! All rows are within the context length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vectordb\n",
    "\n",
    "# start chromdb server\n",
    "# chroma run --host localhost --port 8000 --path ./my_chroma_data\n",
    "\n",
    "embeddings_collection = None\n",
    "def collection_exists(collection_name):\n",
    "    existing_collections = client.list_collections()\n",
    "    return any(collection.name == collection_name for collection in existing_collections)\n",
    "\n",
    "client = chromadb.HttpClient()\n",
    "collection_name = \"cyber-ai-embeddings\"\n",
    "if not collection_exists(collection_name):\n",
    "    embeddings_collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    embeddings_collection = client.get_collection(collection_name)\n",
    "print(\"Collection created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "# >>>>PLEASE READ!!!!!<<<<\n",
    "# THIS CELL EXECUTION SHOULD BE A ONE TIME PROCESS\n",
    "# REMOVE THE ABOVE \"%%SCRIPT TRUE\" TO RUN THIS CELL\n",
    "# THIS CELL PREPARES AND POPULATES ALL THE VECTOR EMBEDDINGS OF THE TRAIN DATA\n",
    "# \n",
    "step = 200\n",
    "max_length = len(prepared_train_data)\n",
    "for i in range(0, max_length, step):\n",
    "    start_idx = i\n",
    "    end_idx = i + step\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    document_ids = []\n",
    "    for index, row in prepared_train_data[start_idx:end_idx].iterrows():\n",
    "        documents.append(row.document)\n",
    "        metadatas.append({\"category\": row.category, \"scategory\": row.sub_category})\n",
    "        document_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=documents)\n",
    "\n",
    "    print(\"inserting results in vectordb\")\n",
    "    embeddings = result[\"embedding\"]\n",
    "    data_to_insert = embeddings_collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=document_ids,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    print(f\"data inserted from index {start_idx} to {end_idx}\")\n",
    "print(\"Vector embeddings prepared...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categories(document,sample_size = 1):\n",
    "    predicted_category = \"\"\n",
    "    predicted_scategory = \"\"\n",
    "    result = genai.embed_content(model=\"models/text-embedding-004\", content=[document])\n",
    "    test_embeddings = result['embedding'][0]\n",
    "    # query db\n",
    "    query_result = embeddings_collection.query(query_embeddings=test_embeddings,n_results=sample_size)\n",
    "    # print results\n",
    "    \n",
    "    sampled_categories = []\n",
    "    sampled_scategories = []\n",
    "    for i in range(sample_size):\n",
    "        if 'category' in query_result['metadatas'][0][0]:\n",
    "            sampled_categories.append(query_result['metadatas'][0][0]['category'])\n",
    "        else:\n",
    "            sampled_categories.append(None)\n",
    "        if 'scategory' in query_result['metadatas'][0][0]:\n",
    "            sampled_scategories.append(query_result['metadatas'][0][0]['scategory'])\n",
    "        else:\n",
    "            sampled_scategories.append(None)\n",
    "            \n",
    "    \n",
    "    # get the most occurred\n",
    "    predicted_category = Counter(sampled_categories).most_common(1)[0][0]\n",
    "    predicted_scategory = Counter(sampled_scategories).most_common(1)[0][0]\n",
    "    return (predicted_category,predicted_scategory,sampled_categories,sampled_scategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Category: Online Financial Fraud\n",
      "Expected Sub-category: Fraud CallVishing\n",
      "Predicted Category: Online Financial Fraud\n",
      "Predicted Sub-category: Fraud CallVishing\n"
     ]
    }
   ],
   "source": [
    "# TEST SINGLE\n",
    "test_idx = int(len(test_data)*random.random())\n",
    "row = test_data.iloc[test_idx]\n",
    "print(f\"Expected Category: {row.category}\")\n",
    "print(f\"Expected Sub-category: {row.sub_category}\")\n",
    "predict_category,predict_scategory,sampled_categories,sampled_scategories  =predict_categories(f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\"\"\",10)\n",
    "\n",
    "print(f\"Predicted Category: {predict_category}\")\n",
    "print(f\"Predicted Sub-category: {predict_scategory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ALL\n",
    "sample_size = 50\n",
    "test_test_data = test_data.sample(n=sample_size).reset_index(drop=True)\n",
    "failure_indexes = []\n",
    "failure_indexes_category = []\n",
    "failure_indexes_scategory = []\n",
    "failure_predicted_categories = []\n",
    "for index, row in test_test_data.iterrows():\n",
    "    print(f\"{index+1}/{sample_size}\",end='\\r')\n",
    "    document = f\"\"\"The following complaint document written by a user is categorized as '{row.category}' and sub-categorized as '{row.sub_category}':\n",
    "{row.crimeaditionalinfo}\"\"\"\n",
    "    predict_category,predict_scategory,sampled_categories,sampled_scategories = predict_categories(document,10)\n",
    "    \n",
    "    if row.category == predict_category:\n",
    "        if row.sub_category == predict_scategory or pd.isna(row.sub_category) or row.sub_category == None:\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            print(\"Failed Sub-category---------\")\n",
    "            print(f\"Index: {index}\")\n",
    "            print(f\"Expected: {row.sub_category}\")\n",
    "            print(f\"Predicted: {predict_scategory}\")\n",
    "            print(\"----------------------------\")\n",
    "            failure_indexes_scategory.append(index)\n",
    "    else:\n",
    "        print(\"Failed Category-------------\")\n",
    "        print(f\"Index: {index}\")\n",
    "        print(f\"Expected: {row.category}\")\n",
    "        print(f\"Predicted: {predict_category}\")\n",
    "        print(\"----------------------------\")\n",
    "        failure_indexes_category.append(index)\n",
    "    \n",
    "    failure_predicted_categories.append([predict_category,predict_scategory])\n",
    "    failure_indexes.append(index)\n",
    "        \n",
    "print(f\"Success Rate Overall: {100 - len(failure_indexes)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Category only): {100 - len(failure_indexes_category)/sample_size*100}%\") \n",
    "print(f\"Success Rate (Sub-category only): {100 - len(failure_indexes_scategory)/sample_size*100}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
